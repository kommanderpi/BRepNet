{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbbb6121",
   "metadata": {},
   "source": [
    "### How to install this package\n",
    "I made this file because I am pretty code illiterate and serves as a reminder \n",
    "\n",
    "1. Make sure you have anaconda installed on your machine\n",
    "2. CLone the github repo https://github.com/kommanderpi/BRepNet.git\n",
    "3. Use 'conda env create -f BRepNet_environment.yml' to isntall all dependencies\n",
    "4. Jupyter notebook for VSCode or similar works well for the visualisation\n",
    "5. Download the raw data using:\n",
    "        cd /path/to/where_you_keep_data/\n",
    "        curl https://fusion-360-gallery-dataset.s3-us-west-2.amazonaws.com/segmentation/s2.0.0/s2.0.0.zip -o s2.0.0.zip\n",
    "        unzip s2.0.0.zip\n",
    "6. Process the data locally using this code in Anaconda powershell(make sure you have activated the conda environment you created using the .yml file) (takes 2-3 hours depending on your device)\n",
    "        python -m pipeline.quickstart --dataset_dir /path/to/where_you_keep_data/ --num_workers 10\n",
    "        we set num_workers to 10 - seems to perform the best locally\n",
    "7. Train the model (takes between 5 and 10 hours depending on your GPU):\n",
    "        python -m train.train --dataset_file /path/to/where_you_keep_data/s2.0.0/processed/dataset.json \\\n",
    "        --dataset_dir /path/to/where_you_keep_data/s2.0.0/processed/ \\\n",
    "        --max_epochs 200 \\\n",
    "        --num_workers=10 \\\n",
    "        --gpus=1\n",
    "8. Train.train will save the best version of your training model under:\n",
    "        /logs/../../checkpoints/filename.ckpt\n",
    "9. Alternatively use the pretrained model provided by Autodesk:\n",
    "        ../example_files/pretrained_models/pretrained_s2.0.0_extended_step_uv_net_features_0816_183419.ckpt\n",
    "\n",
    "10. You can now run the scripts under ../notebooks/filename.ipynb\n",
    "        - BRepNET_Walkthrough.ipynb --> this notebook is a collection of tools and implementation of the segmentation model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51b254fa",
   "metadata": {},
   "source": [
    "### Once we have cloned the repo, downloaded and processed the dataset and trained the model - we can take BRepNet for a test drive\n",
    "We can also skip the training step and use the pretrained model provided in the repo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fd9d2ed",
   "metadata": {},
   "source": [
    "## Generating and Displaying the segmentation results\n",
    "\n",
    "Use a pre-trained BRepNet model, load a STEP model and see predicted segmentation results. You can run this file without training the model first - as Autodesk provides a pretrained model () \n",
    "\n",
    "This notebook should be initiated in the main BRepNet folder.  \n",
    "\n",
    "Improtant notes:\n",
    "- To use these tools on a custom dataset of STEP files: make sure to add a directory and STEP files you want to segment with the model to: ..\\example_files\\< FOLDER_NAME >\n",
    "- You can also add your own files to: ..\\example_files\\step_examples if you want to sample from a larger data set\n",
    "- Run this file in a Jupyter notebook: make sure you are using Python 3.7\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9010f72f",
   "metadata": {},
   "source": [
    "Below we first check that we are in the correct dir and change to the root of the BRepNet repo if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436f36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.isfile('../models/brepnet.py'):\n",
    "    # We are in the notebooks directory.  Change to the root\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89b1507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# This code allows you to evaluate a pre-trained model for all step files in a folder\n",
    "from eval.evaluate_folder import evaluate_folder\n",
    "\n",
    "# This viewer allows you to visualize the results\n",
    "from visualization.jupyter_segmentation_viewer import JupyterSegmentationViewer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c87e3d6",
   "metadata": {},
   "source": [
    "Use `eval/evaluate_folder.py` to evaluate your STEP model. We need to supply the script with the path to the step files, the feature standadization and a pretrained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb69622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "GPU available: True, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed pipeline/extract_feature_data_from_step.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Warning!! - No labels are provided.  This can happen when you\n",
      "are evaluating with a pre-trained model on an unlabelled dataset.\n",
      "Please disregard and accuracy and IoU values which get logged.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dnkmn\\anaconda3\\envs\\brepnet\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1585: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  \"GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\"\n",
      "c:\\Users\\dnkmn\\anaconda3\\envs\\brepnet\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:133: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799fff4fa68a4d68a5836523a9abfc7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test/Chamfer_iou': 0.0,\n",
      " 'test/CutEnd_iou': 1.0,\n",
      " 'test/CutSide_iou': 0.0,\n",
      " 'test/ExtrudeEnd_iou': 0.0,\n",
      " 'test/ExtrudeSide_iou': 0.17142857611179352,\n",
      " 'test/Fillet_iou': 0.0,\n",
      " 'test/RevolveEnd_iou': 0.0,\n",
      " 'test/RevolveSide_iou': 0.0,\n",
      " 'test/accuracy': 0.17142857611179352,\n",
      " 'test/mean_iou': 0.14642857015132904}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a path to your step files:\n",
    "step_folder = Path(\"C:\\BRep_Net\\example_files\\CustomStep\")\n",
    "\n",
    "# We will also need to know the feature standardization for the dataset used to train the model\n",
    "# This is found in the dataset file created by pipeline/build_dataset_file.py or pipeline/quickstart.py\n",
    "feature_standardization = Path(\"./example_files/feature_standardization/s2.0.0_step_all_features.json\")\n",
    "\n",
    "# Here is the path to a pretrained model:\n",
    "pretrained_model = Path(\"./example_files/pretrained_models/pretrained_s2.0.0_extended_step_uv_net_features_0816_183419.ckpt\")\n",
    "\n",
    "# If you want to use your own model, you can uncomment the following line and change the path to your model:\n",
    "\n",
    "# Here is a path to the model we trained ourselves:\n",
    "# pretrained_model = Path(\"C:\\BrepNet\\BRepNet\\example_files\\pretrained_models\\epoch=121-step=13053.ckpt\")\n",
    "\n",
    "# Evaluate the model on these step files.\n",
    "evaluate_folder(step_folder, feature_standardization, pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c27a8a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 3 example files\n",
      "Viewing example 001\n",
      "Viewing example 002\n",
      "Viewing example 003\n"
     ]
    }
   ],
   "source": [
    "logits_folder = Path(\"example_files/CustomStep/temp_working/logits\")\n",
    "step_file_stems = [ f.stem for f in step_folder.glob(\"*.stp\")]\n",
    "print(f\"We found {len(step_file_stems)} example files\")\n",
    "\n",
    "example_index = 0\n",
    "example_index1 = 1\n",
    "example_index2 = 2\n",
    "file_stem = step_file_stems[example_index]\n",
    "print(f\"Viewing example {file_stem}\")\n",
    "file_stem1 = step_file_stems[example_index1]\n",
    "print(f\"Viewing example {file_stem1}\")\n",
    "file_stem2 = step_file_stems[example_index2]\n",
    "print(f\"Viewing example {file_stem2}\")\n",
    "viewer = JupyterSegmentationViewer(file_stem, step_folder, seg_folder=step_folder, logit_folder=logits_folder)\n",
    "viewer1 = JupyterSegmentationViewer(file_stem1, step_folder, seg_folder=step_folder, logit_folder=logits_folder)\n",
    "viewer2 = JupyterSegmentationViewer(file_stem2, step_folder, seg_folder=step_folder, logit_folder=logits_folder)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd35e060",
   "metadata": {},
   "source": [
    "Confirm the shape of your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407a0d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddab5aa55db4a2ea229965422c58169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viewer.view_solid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c270e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb264ca5949404bba4b8d71b58ed6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viewer1.view_solid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026e9b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeae3c6ed8b945419b1dbecdc0493e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viewer2.view_solid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd505370",
   "metadata": {},
   "source": [
    "Predict segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb95f0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc21b7f6e8e4c9084803328373a2436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viewer.view_predicted_segmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f12f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd54f23411e4cb2a78093b9f5a372a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viewer1.view_predicted_segmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "015cd05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253e258081d54ee0b607b9aa999f9d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viewer2.view_predicted_segmentation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48468aad",
   "metadata": {},
   "source": [
    "# Visualizing npz data\n",
    "Once we have extracted BREP data from our STEP files - we can visualize their features in a few different ways.\n",
    "The code blocks below explore different methods to visualize features of a BREP - Faces, Edges, Edge Direction, Coedges and Coedge Tagents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427d8c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dnkmn\\anaconda3\\envs\\brepnet\\python.exe\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import os\n",
    "if os.path.isfile(\"../models/brepnet.py\"):\n",
    "    os.chdir(\"../\")\n",
    "import tempfile\n",
    "import numpy as np\n",
    "    \n",
    "import utils.data_utils as data_utils\n",
    "\n",
    "# Imports from occwl\n",
    "from occwl.io import load_step\n",
    "from occwl.jupyter_viewer import JupyterViewer\n",
    "from occwl.entity_mapper import EntityMapper\n",
    "\n",
    "\n",
    "# Imports from BRepNet\n",
    "from pipeline.extract_brepnet_data_from_step import extract_brepnet_data_from_step\n",
    "import utils.scale_utils as scale_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1103e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the path to some example step files for us to convert\n",
    "step_folder = Path(\"./example_files/step_examples\")\n",
    "temp_folder = Path(tempfile.gettempdir())\n",
    "working = temp_folder / \"brepnet_test_working_dir\"\n",
    "if not working.exists():\n",
    "    working.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc492ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [03:29<00:00,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed pipeline/extract_feature_data_from_step.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we generate npz data from step files\n",
    "extract_brepnet_data_from_step(step_folder, working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd02518f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of step and npz files 25\n",
      "Viewing file\n",
      "npz data C:\\Users\\dnkmn\\AppData\\Local\\Temp\\brepnet_test_working_dir\\100155_57ec5fc6_0.npz\n",
      "STEP data example_files\\step_examples\\100155_57ec5fc6_0.stp\n",
      "Loaded 1 solids\n"
     ]
    }
   ],
   "source": [
    "all_npz_files = [ f for f in working.glob(\"*.npz\") ]\n",
    "npz_files = []\n",
    "step_files = []\n",
    "for npz_file in all_npz_files:\n",
    "    step_file = step_folder / (npz_file.stem + \".stp\")\n",
    "    if step_file.exists():\n",
    "        npz_files.append(npz_file)\n",
    "        step_files.append(step_file)\n",
    "    \n",
    "print(f\"Number of step and npz files {len(npz_files)}\")\n",
    "\n",
    "file_index = 0\n",
    "npz_file = npz_files[file_index]\n",
    "step_file = step_folder / (npz_file.stem + \".stp\")\n",
    "print(\"Viewing file\") \n",
    "print(f\"npz data {npz_file}\")\n",
    "print(f\"STEP data {step_file}\")\n",
    "\n",
    "# Load the solid\n",
    "solids = load_step(step_file)\n",
    "print(f\"Loaded {len(solids)} solids\")\n",
    "solid = solids[0]\n",
    "\n",
    "# Scale to [-1,1]^3 box\n",
    "solid = scale_utils.scale_solid_to_unit_box(solid)\n",
    "\n",
    "# Load the npz data\n",
    "data = data_utils.load_npz_data(npz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe95c020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b018bd0d334cfc9cb1105dbb48e367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viewer = JupyterViewer()\n",
    "for face in solid.faces():\n",
    "    viewer.display(face, render_edges=True)\n",
    "viewer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b210ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The faces you selected were [39 35 36]\n",
      "Shape of face grids (44, 7, 10, 10)\n",
      "Shape of selected face grids (3, 7, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "if len(viewer.selected_faces()) <= 0:\n",
    "    print(\"Select some faces to view\")\n",
    "else:\n",
    "    entity_mapper = EntityMapper(solid)\n",
    "    selected_face_indices = viewer.selected_face_indices(entity_mapper)\n",
    "    print(f\"The faces you selected were {selected_face_indices}\")\n",
    "\n",
    "    # Get the point grids from the data\n",
    "    face_grids = data[\"face_point_grids\"]\n",
    "    print(f\"Shape of face grids {face_grids.shape}\")\n",
    "    selected_face_grids = face_grids[selected_face_indices]\n",
    "    print(f\"Shape of selected face grids {selected_face_grids.shape}\")\n",
    "    selected_face_grid_points = selected_face_grids[:,:3,:,:]\n",
    "    points = np.transpose(np.reshape(selected_face_grid_points, (3, -1)))\n",
    "    viewer.display_points(points)\n",
    "    selected_face_grid_normals  = selected_face_grids[:,3:6,:,:]\n",
    "    normals = np.transpose(np.reshape(selected_face_grid_normals, (3, -1)))\n",
    "    viewer.display_unit_vectors(points, normals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1b6bbcd",
   "metadata": {},
   "source": [
    "# Edge reverse flag as color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66829a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd67f51e307e429d9c9ee1a507ec9e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "edge_color_viewer = JupyterViewer()\n",
    "edge_color_viewer.display(solid, render_edges=False)\n",
    "for edge in solid.edges():\n",
    "    if edge.reversed():\n",
    "        color = \"red\"\n",
    "    else:\n",
    "        color = \"blue\"\n",
    "    edge_color_viewer.display(edge, edge_color=color)\n",
    "edge_color_viewer.show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "688266f6",
   "metadata": {},
   "source": [
    "# Edge reverse flag as direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee797a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e75d0acdc349ca9269eb089df28068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_dir_across_edge(viewer, coedge_grid):\n",
    "    points = np.transpose(coedge_grid[:3, :])\n",
    "    tangents = np.transpose(coedge_grid[3:6, :])\n",
    "    left_normals = np.transpose(coedge_grid[6:9, :])\n",
    "    across_edge_dir = np.cross(tangents, left_normals)\n",
    "    viewer.display_points(points)\n",
    "    viewer.display_unit_vectors(points, across_edge_dir, line_color=\"blue\")\n",
    "\n",
    "    \n",
    "edge_dir_viewer = JupyterViewer()\n",
    "edge_dir_viewer.display(solid, render_edges=True)\n",
    "entity_mapper = EntityMapper(solid)\n",
    "coedge_grids = data[\"coedge_point_grids\"]\n",
    "for edge in solid.edges():\n",
    "    if edge.reversed():\n",
    "        coedge = edge.reversed_edge()\n",
    "    else:\n",
    "        coedge = edge\n",
    "    coedge_index = entity_mapper.oriented_edge_index(coedge)\n",
    "    coedge_grid = coedge_grids[coedge_index]\n",
    "    display_dir_across_edge(edge_dir_viewer, coedge_grid)\n",
    "    \n",
    "edge_dir_viewer.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45ba494e",
   "metadata": {},
   "source": [
    "# Coedge grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5edd84a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d95120d0e2d40318aad933604b3a871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coedge_viewer = JupyterViewer()\n",
    "for face in solid.faces():\n",
    "    coedge_viewer.display(face, render_edges=True)\n",
    "coedge_viewer.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0ac77a5",
   "metadata": {},
   "source": [
    "Select two adjacent faces so that we can visualize the coedge grid for the common edge.  The coedge is on the side of the first face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "30fac338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 selected faces\n",
      "Face 0: 1\n",
      "Face 1: 39\n",
      "Found edges which shares faces\n",
      "Coedge index 10\n"
     ]
    }
   ],
   "source": [
    "def display_coedge_grid(viewer, grid):\n",
    "    points = np.transpose(grid[:3, :])\n",
    "    tangents = np.transpose(grid[3:6, :])\n",
    "    left_normals = np.transpose(grid[6:9, :])\n",
    "    right_normals = np.transpose(grid[9:12, :])\n",
    "    viewer.display_points(points)\n",
    "    viewer.display_unit_vectors(points, tangents, line_color=\"red\")\n",
    "    viewer.display_unit_vectors(points, left_normals, line_color=\"blue\")\n",
    "    viewer.display_unit_vectors(points, right_normals, line_color=\"green\")\n",
    "    \n",
    "def find_selected_coedge(viewer, entity_mapper):\n",
    "    selected_faces = viewer.selected_faces()\n",
    "    for index, selected_face in enumerate(selected_faces):\n",
    "        selected_face_index = entity_mapper.face_index(selected_face)\n",
    "        print(f\"Face {index}: {selected_face_index}\")\n",
    "        \n",
    "    selected_faces_set = set(selected_faces)\n",
    "       \n",
    "    # Find the coedge to the left of the face 0 and right of face 1\n",
    "    target_coedge = None\n",
    "    for coedge in solid.edges():\n",
    "        ffe = set(solid.faces_from_edge(coedge))\n",
    "        if ffe == selected_faces_set:\n",
    "            print(\"Found edges which shares faces\")\n",
    "            mate = coedge.reversed_edge()\n",
    "            if len(selected_faces) == 2:\n",
    "                if selected_faces[0].is_left_of(coedge) and selected_faces[1].is_left_of(mate):\n",
    "                    target_coedge = coedge\n",
    "                elif selected_faces[0].is_left_of(mate) and selected_faces[1].is_left_of(coedge):\n",
    "                    target_coedge = mate\n",
    "            elif len(selected_faces) == 1:\n",
    "                assert False, \"Need to implement\"\n",
    "    return target_coedge\n",
    "    \n",
    "num_selected_faces = len(coedge_viewer.selected_faces())\n",
    "print(f\"Found {num_selected_faces} selected faces\")\n",
    "if num_selected_faces > 0:\n",
    "    entity_mapper = EntityMapper(solid)\n",
    "    target_coedge = find_selected_coedge(coedge_viewer, entity_mapper)\n",
    "            \n",
    "    if target_coedge is not None:\n",
    "        coedge_index = entity_mapper.oriented_edge_index(target_coedge)\n",
    "        print(f\"Coedge index {coedge_index}\")\n",
    "        coedge_grids = data[\"coedge_point_grids\"]\n",
    "        selected_coedge_grids = coedge_grids[coedge_index]\n",
    "        display_coedge_grid(coedge_viewer, selected_coedge_grids)\n",
    "    else:\n",
    "        print(\"Found no coedge\")\n",
    "else:\n",
    "    print(\"Please select one face and one edge\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56ee54b1",
   "metadata": {},
   "source": [
    "# Coedge LCS matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "72c4a67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a0db36314c48899d22b5503cf65d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lcs_viewer = JupyterViewer()\n",
    "for face in solid.faces():\n",
    "    lcs_viewer.display(face, render_edges=True)\n",
    "lcs_viewer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a61ecb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 selected faces\n",
      "Face 0: 34\n",
      "Face 1: 1\n",
      "Found edges which shares faces\n",
      "Coedge index 174\n"
     ]
    }
   ],
   "source": [
    "def display_coedge_lcs(viewer, selected_coedge_lcs):\n",
    "    origin = np.expand_dims(selected_coedge_lcs[:, 3][:3], axis=0)\n",
    "    u_vec = np.expand_dims(selected_coedge_lcs[:, 0][:3], axis=0)\n",
    "    v_vec = np.expand_dims(selected_coedge_lcs[:, 1][:3], axis=0)\n",
    "    w_vec = np.expand_dims(selected_coedge_lcs[:, 2][:3], axis=0)\n",
    "    length = 0.02\n",
    "    u_end = origin + u_vec*length\n",
    "    v_end = origin + v_vec*length\n",
    "    w_end = origin + w_vec*length\n",
    "    viewer.display_points(origin)\n",
    "    viewer.display_lines(origin, u_end, line_width=2, line_color=\"red\")\n",
    "    viewer.display_lines(origin, v_end, line_width=2, line_color=\"green\")\n",
    "    viewer.display_lines(origin, w_end, line_width=2, line_color=\"blue\")\n",
    "\n",
    "num_selected_faces = len(lcs_viewer.selected_faces())\n",
    "print(f\"Found {num_selected_faces} selected faces\")\n",
    "if num_selected_faces > 0:\n",
    "    entity_mapper = EntityMapper(solid)\n",
    "    target_coedge = find_selected_coedge(lcs_viewer, entity_mapper)\n",
    "            \n",
    "    if target_coedge is not None:\n",
    "        coedge_index = entity_mapper.oriented_edge_index(target_coedge)\n",
    "        print(f\"Coedge index {coedge_index}\")\n",
    "        coedge_lcs = data[\"coedge_lcs\"]\n",
    "        selected_coedge_lcs = coedge_lcs[coedge_index]\n",
    "        display_coedge_lcs(lcs_viewer, selected_coedge_lcs)\n",
    "    else:\n",
    "        print(\"Found no coedge\")\n",
    "else:\n",
    "    print(\"Please select one face and one edge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05be0d",
   "metadata": {},
   "source": [
    "# View edge grids which UV-Net sees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6267301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f01faaf8f44d979527af4f750a1db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uv_net_edge_viewer = JupyterViewer()\n",
    "uv_net_edge_viewer.display(solid, render_edges=True)\n",
    "entity_mapper = EntityMapper(solid)\n",
    "coedge_grids = data[\"coedge_point_grids\"]\n",
    "for edge in solid.edges():\n",
    "    coedge_index = entity_mapper.oriented_edge_index(edge)\n",
    "    coedge_grid = coedge_grids[coedge_index]\n",
    "    display_coedge_grid(uv_net_edge_viewer, coedge_grid)\n",
    "uv_net_edge_viewer.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb759d8f",
   "metadata": {},
   "source": [
    "# Edge tangent directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "18776990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3340549fda4800ba196adfca22cdc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_edge_tangents(viewer, grid):\n",
    "    points = np.transpose(grid[:3, [0, 2, 4, 6, 8]])\n",
    "    \n",
    "    view_dir = np.array([1,1,-1])\n",
    "    \n",
    "    # Find a good length for the arrows\n",
    "    mins = np.min(points, axis=0)\n",
    "    maxs = np.max(points, axis=0)\n",
    "    diag = maxs - mins\n",
    "    longest = np.max(diag)\n",
    "    arrow_length = longest/8   \n",
    "    arrow_head = arrow_length/4\n",
    "    \n",
    "    points += view_dir*arrow_head\n",
    "    \n",
    "    tangents = np.transpose(grid[3:6, [0, 2, 4, 6, 8]])\n",
    "    arrow_ends = points + arrow_length*tangents\n",
    "    arrow_dir_cross = view_dir/np.linalg.norm(view_dir)\n",
    "    arrow_offset1 = np.cross(tangents, arrow_dir_cross)\n",
    "    arrow_offset2 = -arrow_offset1\n",
    "    arrow1 = arrow_ends - (tangents+arrow_offset1)*arrow_head\n",
    "    arrow2 = arrow_ends - (tangents+arrow_offset2)*arrow_head\n",
    "    line_width = 2\n",
    "    viewer.display_lines(points, arrow_ends, line_color=\"blue\", line_width=line_width)\n",
    "    viewer.display_lines(arrow1, arrow_ends, line_color=\"blue\", line_width=line_width)\n",
    "    viewer.display_lines(arrow2, arrow_ends, line_color=\"blue\", line_width=line_width)\n",
    "    \n",
    "edge_tangent_viewer = JupyterViewer()\n",
    "edge_tangent_viewer.display(solid, render_edges=True)\n",
    "entity_mapper = EntityMapper(solid)\n",
    "coedge_grids = data[\"coedge_point_grids\"]\n",
    "for edge in solid.edges():\n",
    "    if edge.reversed():\n",
    "        # If the edge had the reverse flag set then the code in \n",
    "        # the EdgeDataExtractor already reversed the tangent directions.\n",
    "        # To get the same tangent directions as the edges 3d curve\n",
    "        # we need to get the coedge grid from the mate\n",
    "        coedge = edge.reversed_edge()\n",
    "    else:\n",
    "        coedge = edge\n",
    "    coedge_index = entity_mapper.oriented_edge_index(coedge)\n",
    "    coedge_grid = coedge_grids[coedge_index]\n",
    "    display_edge_tangents(edge_tangent_viewer, coedge_grid)\n",
    "edge_tangent_viewer.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e1bed07",
   "metadata": {},
   "source": [
    "# Now that we understand how BRepNet reads and analyzes STEP files - lets explore how BRepNet can be used to find similarities between STEP files "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6159a7b0",
   "metadata": {},
   "source": [
    "## Using the BRepNet input features as a face and edge similarity metric\n",
    "BRepNet input features can be used as a similarity metric for finding similar faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c97f8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "if os.path.isfile('../models/brepnet.py'):\n",
    "    # We are in the notebooks directory.  Change to the root\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a78c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# This viewer allows you to visualize the results\n",
    "from visualization.jupyter_segmentation_viewer import JupyterSegmentationViewer\n",
    "\n",
    "# Useful functions for loading data\n",
    "import utils.data_utils as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eec41417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the path to some example step files for us to convert\n",
    "# step_folder = Path(\"./example_files/CustomStep\")\n",
    "step_folder = Path(\"./example_files/step_examples\")\n",
    "\n",
    "# We will also need to know the feature standardization for the dataset used to train the model\n",
    "# This is found in the dataset file created by pipeline/build_dataset_file.py or pipeline/quickstart.py\n",
    "feature_standardization_path = Path(\"./example_files/feature_standardization/s2.0.0_step_all_features.json\")\n",
    "standardization_data = data_utils.load_json_data(feature_standardization_path)\n",
    "feature_standardization = standardization_data[\"feature_standardization\"]\n",
    "\n",
    "# This function to standardize features is similar to what we do in the dataloader\n",
    "def standardize_features(feature_tensor, stats):\n",
    "    num_features = len(stats)\n",
    "    assert feature_tensor.shape[1] == num_features\n",
    "    means = np.zeros(num_features)\n",
    "    sds = np.zeros(num_features)\n",
    "    eps = 1e-7\n",
    "    for index, s in enumerate(stats):\n",
    "        assert s[\"standard_deviation\"] > eps, \"Feature has zero standard deviation\"\n",
    "        means[index] = s[\"mean\"]\n",
    "        sds[index] = s[\"standard_deviation\"]\n",
    "\n",
    "    # We need to broadcast means and sds over the number of entities\n",
    "    means_x = np.expand_dims(means, axis=0)\n",
    "    sds_x = np.expand_dims(sds, axis=0)\n",
    "    feature_tensor_zero_mean = feature_tensor - means_x\n",
    "    feature_tensor_standadized = feature_tensor_zero_mean / sds_x\n",
    "\n",
    "    # Convert the tensors to floats after standardization \n",
    "    return feature_tensor_standadized\n",
    "    \n",
    "def standarize_data(data, feature_standardization):\n",
    "    data[\"face_features\"] = standardize_features(data[\"face_features\"], feature_standardization[\"face_features\"])\n",
    "    data[\"edge_features\"] = standardize_features(data[\"edge_features\"], feature_standardization[\"edge_features\"])\n",
    "    data[\"coedge_features\"] = standardize_features(data[\"coedge_features\"], feature_standardization[\"coedge_features\"])\n",
    "    return data\n",
    "    \n",
    "def find_faces_to_edges(coedge_to_face, coedge_to_edge):\n",
    "    faces_to_edges_dict = {}\n",
    "    for coedge_index in range(coedge_to_face.shape[0]):\n",
    "        edge = coedge_to_edge[coedge_index]\n",
    "        face = coedge_to_face[coedge_index]\n",
    "        if not face in faces_to_edges_dict:\n",
    "            faces_to_edges_dict[face] = set()\n",
    "        faces_to_edges_dict[face].add(edge)\n",
    "    faces_to_edges = []\n",
    "    for i in range(len(faces_to_edges_dict)):\n",
    "        assert i in faces_to_edges_dict\n",
    "        faces_to_edges.append(faces_to_edges_dict[i])\n",
    "    return faces_to_edges\n",
    "\n",
    "# This function pools edges features onto all faces which\n",
    "# are adjacent to that edge\n",
    "def pool_edge_data_onto_faces(data):\n",
    "    face_features = data[\"face_features\"]\n",
    "    edge_features = data[\"edge_features\"]\n",
    "    coedge_to_face = data[\"coedge_to_face\"] \n",
    "    coedge_to_edge = data[\"coedge_to_edge\"]\n",
    "    for edge in coedge_to_edge:\n",
    "        assert edge < edge_features.shape[0]\n",
    "    faces_to_edges = find_faces_to_edges(coedge_to_face, coedge_to_edge)\n",
    "    face_edge_features = []\n",
    "    for face_edge_set in faces_to_edges:\n",
    "        edge_features_for_face  = []\n",
    "        for edge in face_edge_set:\n",
    "            edge_features_for_face.append(edge_features[edge])\n",
    "        pooled_edge_features = np.max(np.stack(edge_features_for_face), axis = 0)\n",
    "        face_edge_features.append(pooled_edge_features)\n",
    "    assert len(face_edge_features) == face_features.shape[0]\n",
    "    face_edge_features = np.stack(face_edge_features)\n",
    "    return np.concatenate([face_features, face_edge_features], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619902b1",
   "metadata": {},
   "source": [
    "Choose the model you would like to use as the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35701938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 25 example files\n"
     ]
    }
   ],
   "source": [
    "step_file_stems = [ f.stem for f in step_folder.glob(\"*.stp\")]\n",
    "print(f\"We found {len(step_file_stems)} example files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c172d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_index = 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e09c84f",
   "metadata": {},
   "source": [
    "Select some faces on the solid to continue.  \n",
    "Double click on each face to select it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f3e9fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewing example 24051_4852a192_5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c28ba60e46547f4820c4d3e9cf3a1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_stem = step_file_stems[query_index]\n",
    "print(f\"Viewing example {file_stem}\")\n",
    "viewer = JupyterSegmentationViewer(file_stem, step_folder, seg_folder=step_folder)\n",
    "viewer.view_solid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e30e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(viewer.selection_list) > 0, \"Please select some faces on the solid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52dd9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw feature data\n",
    "npz_folder = step_folder / \"temp_working\"\n",
    "npz_pathname = npz_folder / (file_stem + \".npz\")\n",
    "data = data_utils.load_npz_data(npz_pathname)\n",
    "data = standarize_data(data, feature_standardization)\n",
    "face_features = data[\"face_features\"]\n",
    "edge_features = data[\"edge_features\"]\n",
    "pooled_face_edge_features = pool_edge_data_onto_faces(data)\n",
    "\n",
    "assert pooled_face_edge_features.shape[0] == len(viewer.entity_mapper.face_map), \"Embedding size doesn't match solid\"\n",
    "selected_faces_features = pooled_face_edge_features[viewer.selection_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "703c515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = list(npz_folder.glob(\"*.npz\"))\n",
    "\n",
    "min_dists_for_each_face = []\n",
    "sum_min_dists_for_each_solid = []\n",
    "for npz_file in all_files:\n",
    "        \n",
    "    # Load the input features\n",
    "    data = data_utils.load_npz_data(npz_file)\n",
    "    data = standarize_data(data, feature_standardization)\n",
    "    pooled_face_edge_features = pool_edge_data_onto_faces(data)\n",
    "        \n",
    "    min_dists = []\n",
    "    min_dists_for_each_face_in_this_solid = []\n",
    "    for selected_face_features in selected_faces_features:\n",
    "        vec = pooled_face_edge_features-selected_face_features\n",
    "        dist = np.linalg.norm(vec, axis=1)\n",
    "        \n",
    "        # Here we compute the distance from a given query face\n",
    "        # to each face in the solid\n",
    "        min_dists_for_each_face_in_this_solid.append(dist)\n",
    "        \n",
    "        # Then is metric is the distance from that given query face to\n",
    "        # the most similar face in the solid\n",
    "        min_dists.append(np.min(dist))\n",
    "    \n",
    "    # Now we need some way to say which solids is the best match over all\n",
    "    # We do this by summing the minimum distances to all the faces \n",
    "    sum_dists = np.sum(np.stack(min_dists), axis=0)\n",
    "    sum_min_dists_for_each_solid.append(sum_dists)\n",
    "    \n",
    "    # For display we also keep track of the min distance to each face\n",
    "    min_dists_for_each_face_in_this_solid = np.min(np.stack(min_dists_for_each_face_in_this_solid), axis=0)\n",
    "    min_dists_for_each_face.append(min_dists_for_each_face_in_this_solid)\n",
    "    \n",
    "sum_min_dists_for_each_solid = np.array(sum_min_dists_for_each_solid)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19014119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval [0.0, 7.467671834088819]\n"
     ]
    }
   ],
   "source": [
    "# Now we will search for the top k best matches\n",
    "k = 5\n",
    "indices_of_smallest = np.argpartition(sum_min_dists_for_each_solid, kth=range(k))[:k]\n",
    "\n",
    "# We want to show how close each face in each solid is\n",
    "# To do this we want some kind of range which is computed here\n",
    "all_dists_top_k = []\n",
    "for index in indices_of_smallest:\n",
    "    all_dists_top_k.append(min_dists_for_each_face[index])\n",
    "all_dists_top_k = np.concatenate(all_dists_top_k)\n",
    "interval = [all_dists_top_k.min(), all_dists_top_k.max()]\n",
    "print(f\"Interval {interval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82da1ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "Close file 24051_4852a192_5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf5257d7f304256a1a883fc6b7e8406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "Close file 97826_238f01e7_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6e73b216ce429f8401eb7f6b97cc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Close file 21492_8bd34fc1_15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7014919407ad475d81b940703a97f723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Close file 24013_13d47495_3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c739498cc284d6ebb38edd26746746f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Close file 42811_d1f01a68_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb439046299444f6aca3fde90b73aa17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now as a sanity check we always expect to find the query as the best \n",
    "# match.  The other matching faces (red) should have a similar shape\n",
    "for i, index in enumerate(indices_of_smallest):\n",
    "    print(index)\n",
    "    close_file_stem = all_files[index].stem\n",
    "    print(f\"Close file {close_file_stem}\")\n",
    "    close_viewer = JupyterSegmentationViewer(close_file_stem, step_folder)\n",
    "    dists_to_view = min_dists_for_each_face[index]\n",
    "    close_viewer.display_faces_with_heatmap(dists_to_view, interval)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97dfb124",
   "metadata": {},
   "source": [
    "## Using BRepNet embeddings for face similarity matching\n",
    "We can use the embeddings generated by the BRepNet model can be used for selecitng similar faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df31152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "if os.path.isfile('../models/brepnet.py'):\n",
    "    # We are in the notebooks directory.  Change to the root\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699bfd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# This code allows you to evaluate a pre-trained model for all step files in a folder\n",
    "from eval.evaluate_folder import evaluate_folder\n",
    "\n",
    "# This viewer allows you to visualize the results\n",
    "from visualization.jupyter_segmentation_viewer import JupyterSegmentationViewer\n",
    "\n",
    "from visualization.save_images_of_similar_solids import SimilarSolidImageSaver\n",
    "from occwl.solid import Solid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91e71bd7",
   "metadata": {},
   "source": [
    "To evaluate a collection of STEP files we need to use `eval/evaluate_folder.py`.  \n",
    "\n",
    "Provide the the script with the path to the step files to evaluate, the feature standadization and the pretrained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f741bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed pipeline/extract_feature_data_from_step.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dnkmn\\anaconda3\\envs\\brepnet\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1585: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  \"GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using labels from example_files\\step_examples\\temp_working\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dnkmn\\anaconda3\\envs\\brepnet\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:133: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c777062f896f4ab8bbb5ad7178281515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test/Chamfer_iou': 0.8418079018592834,\n",
      " 'test/CutEnd_iou': 0.7290322780609131,\n",
      " 'test/CutSide_iou': 0.8102856874465942,\n",
      " 'test/ExtrudeEnd_iou': 0.7076271176338196,\n",
      " 'test/ExtrudeSide_iou': 0.8099502325057983,\n",
      " 'test/Fillet_iou': 0.9281437397003174,\n",
      " 'test/RevolveEnd_iou': 0.8999999761581421,\n",
      " 'test/RevolveSide_iou': 0.7386363744735718,\n",
      " 'test/accuracy': 0.8996027708053589,\n",
      " 'test/mean_iou': 0.8081854581832886}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Add the path to the dataset folder containng the step files:\n",
    "# step_folder = Path(\"./example_files/CustomStep\")\n",
    "step_folder = Path(\"./example_files/step_examples\")\n",
    "\n",
    "\n",
    "\n",
    "# You can run this on the entire extended STEP dataset which you can doanload from \n",
    "# https://fusion-360-gallery-dataset.s3.us-west-2.amazonaws.com/segmentation/s2.0.0/s2.0.0_extended_step.zip\n",
    "# step_folder = Path(\"/path/to/s2.0.0_extended_step/breps/step/\")\n",
    "\n",
    "# Provide the feature standardization for the dataset used to train the model\n",
    "feature_standardization = Path(\"./example_files/feature_standardization/s2.0.0_step_all_features.json\")\n",
    "\n",
    "# Provide a pretrained model\n",
    "pretrained_model = Path(\"./example_files/pretrained_models/pretrained_s2.0.0_extended_step_uv_net_features_0816_183419.ckpt\")\n",
    "\n",
    "# This function evaluates the model on your step files.\n",
    "evaluate_folder(step_folder, feature_standardization, model=pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6423e9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 25 example files\n"
     ]
    }
   ],
   "source": [
    "step_file_stems = [ f.stem for f in step_folder.glob(\"*.stp\")]\n",
    "print(f\"We found {len(step_file_stems)} example files\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a3f55bf",
   "metadata": {},
   "source": [
    "Select the model that you want to use as a base reference for your query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c46b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_index = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d074ab72",
   "metadata": {},
   "source": [
    "Select faces on the solid before proceeding to the next step (Double click on a face to select it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2439c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewing example 21492_8bd34fc1_15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930922c431674d4f966bb2092a02d564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_stem = step_file_stems[query_index]\n",
    "print(f\"Viewing example {file_stem}\")\n",
    "viewer = JupyterSegmentationViewer(file_stem, step_folder, seg_folder=step_folder)\n",
    "viewer.view_solid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f15b8f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(viewer.selection_list) > 0, \"Please select some faces on the solid\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a358c7b7",
   "metadata": {},
   "source": [
    "Load the embeddings for the query solid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ca3c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings for the query solid\n",
    "embeddings_folder = step_folder / \"temp_working/embeddings\"\n",
    "embeddings_pathname = embeddings_folder / (file_stem + \".embeddings\")\n",
    "embeddings = np.loadtxt(embeddings_pathname)\n",
    "assert embeddings.shape[0] == len(viewer.entity_mapper.face_map), \"Embedding size doesn't match solid\"\n",
    "selected_face_embeddings = embeddings[viewer.selection_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aac2cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all solids and find the distance from each face of each solid to each query face\n",
    "all_embeddings = list(embeddings_folder.glob(\"*.embeddings\"))\n",
    "min_dists_for_each_face = []\n",
    "sum_min_dists_for_each_solid = []\n",
    "for embedding_file in all_embeddings:\n",
    "    target_file_stem = embedding_file.stem\n",
    "    \n",
    "    # Load the embeddings for the target file\n",
    "    embeddings = np.loadtxt(embedding_file)\n",
    "    if len(embeddings.shape) == 1:\n",
    "        embeddings = np.expand_dims(embeddings, axis=0)\n",
    "        \n",
    "    min_dists = []\n",
    "    min_dists_for_each_face_in_this_solid = []\n",
    "    for face_embedding in selected_face_embeddings:\n",
    "        face_to_embeddings = embeddings-face_embedding\n",
    "        dist = np.linalg.norm(face_to_embeddings, axis=1)\n",
    "        \n",
    "        # Compute the distance from a given query face to each face in the solid\n",
    "        min_dists_for_each_face_in_this_solid.append(dist)\n",
    "        \n",
    "        # A metric is generated that represents the distance from a given query face to the most similar face in the solid\n",
    "        min_dists.append(np.min(dist))\n",
    "    \n",
    "    # To say which solids is the best match overall - we sum the minimum distances to all the faces \n",
    "    sum_dists = np.sum(np.stack(min_dists), axis=0)\n",
    "    sum_min_dists_for_each_solid.append(sum_dists)\n",
    "    \n",
    "    # For display purposes, we keep track of the min distance to each face\n",
    "    min_dists_for_each_face_in_this_solid = np.min(np.stack(min_dists_for_each_face_in_this_solid), axis=0)\n",
    "    min_dists_for_each_face.append(min_dists_for_each_face_in_this_solid)\n",
    "    \n",
    "sum_min_dists_for_each_solid = np.array(sum_min_dists_for_each_solid)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1928a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval [0.0, 19.19418612741993]\n"
     ]
    }
   ],
   "source": [
    "# Now we will search for the top k best matches\n",
    "k = 4\n",
    "indices_of_smallest = np.argpartition(sum_min_dists_for_each_solid, kth=range(k))[:k]\n",
    "\n",
    "# We want to show how close each face in each solid is we define a range.\n",
    "all_dists_top_k = []\n",
    "for index in indices_of_smallest:\n",
    "    all_dists_top_k.append(min_dists_for_each_face[index])\n",
    "all_dists_top_k = np.concatenate(all_dists_top_k)\n",
    "interval = [all_dists_top_k.min(), all_dists_top_k.max()]\n",
    "print(f\"Interval {interval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddaa1f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Close file 21492_8bd34fc1_15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa5bc18ff1e49188ee251d9bca17e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "Close file 44647_d83249a9_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5372fc851647109f159cba30ab662d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "Close file 97826_238f01e7_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9686c40580450b94311a55cbf6de08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "Close file 56436_2a8fc254_3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ecf11ab64a4d8fbfb161b459d3bf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# As a sanity check we always expect to find the query as the best match. All other matching faces (red) should have a similar shape\n",
    "for i, index in enumerate(indices_of_smallest):\n",
    "    print(index)\n",
    "    close_file_stem = all_embeddings[index].stem\n",
    "    print(f\"Close file {close_file_stem}\")\n",
    "    close_viewer = JupyterSegmentationViewer(close_file_stem, step_folder)\n",
    "    dists_to_view = min_dists_for_each_face[index]\n",
    "    close_viewer.display_faces_with_heatmap(dists_to_view, interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
